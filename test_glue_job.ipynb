{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9f9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è AWS Glue Libraries NOT found. Using MOCK objects for Local Laptop.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- SETUP MOCK ENVIRONMENT FOR LOCAL RUN ---\n",
    "try:\n",
    "    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° Import ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡∏ö‡∏ô Glue Docker ‡∏´‡∏£‡∏∑‡∏≠ Cloud)\n",
    "    from awsglue.context import GlueContext\n",
    "    from awsglue.job import Job\n",
    "    from awsglue.utils import getResolvedOptions\n",
    "    print(\"‚úÖ Found AWS Glue Libraries (Running on AWS/Docker)\")\n",
    "except ImportError:\n",
    "    # ‡∏ñ‡πâ‡∏≤‡∏£‡∏±‡∏ô‡∏ö‡∏ô Laptop ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á Mock Class ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÅ‡∏ó‡∏ô\n",
    "    print(\"‚ö†Ô∏è AWS Glue Libraries NOT found. Using MOCK objects for Local Laptop.\")\n",
    "    \n",
    "    # 1. Mock GlueContext\n",
    "    class GlueContext:\n",
    "        def __init__(self, sc):\n",
    "            self.sc = sc\n",
    "            # ‡πÉ‡∏ä‡πâ Local SparkSession ‡πÅ‡∏ó‡∏ô Glue SparkSession\n",
    "            self.spark_session = SparkSession.builder \\\n",
    "                .master(\"local[*]\") \\\n",
    "                .appName(\"LocalGlueRun\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "    # 2. Mock Job Class\n",
    "    class Job:\n",
    "        def __init__(self, glue_context):\n",
    "            pass\n",
    "        def init(self, job_name, args):\n",
    "            print(f\"   -> [Mock] Job Initialized: {job_name}\")\n",
    "        def commit(self):\n",
    "            print(\"   -> [Mock] Job Committed\")\n",
    "\n",
    "    # 3. Mock getResolvedOptions\n",
    "    def getResolvedOptions(args, options):\n",
    "        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Dictionary ‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤ Mock ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà Key ‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏°‡∏≤\n",
    "        return {k: 'mock_value' for k in options}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a918133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/23 13:50:56 WARN Utils: Your hostname, Santis-Mac-Studio.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.38 instead (on interface en0)\n",
      "26/01/23 13:50:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/23 13:50:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> [Mock] Job Initialized: local_notebook_test\n",
      "--- Starting Job with 3 Max Threads ---\n",
      "\n",
      "\n",
      "üì¢ PRIORITY GROUP: 1\n",
      "------------------------------\n",
      "üëâ Running Parallel (2 jobs)...\n",
      "   ‚ñ∂ [Start] users (P:1)\n",
      "   ‚ñ∂ [Start] products (P:1)\n",
      "   ‚úÖ [Done] users (1.68s)\n",
      "   ‚úÖ [Done] products (1.76s)\n",
      "\n",
      "üì¢ PRIORITY GROUP: 2\n",
      "------------------------------\n",
      "üëâ Running Sequential (1 jobs)...\n",
      "   ‚ñ∂ [Start] orders (P:2)\n",
      "   ‚úÖ [Done] orders (2.13s)\n",
      "üëâ Running Parallel (3 jobs)...\n",
      "   ‚ñ∂ [Start] logs_a (P:2)\n",
      "   ‚ñ∂ [Start] logs_b (P:2)\n",
      "   ‚ñ∂ [Start] logs_c (P:2)\n",
      "   ‚úÖ [Done] logs_a (1.67s)\n",
      "   ‚úÖ [Done] logs_b (2.62s)\n",
      "   ‚úÖ [Done] logs_c (2.83s)\n",
      "\n",
      "üì¢ PRIORITY GROUP: 3\n",
      "------------------------------\n",
      "üëâ Running Parallel (1 jobs)...\n",
      "   ‚ñ∂ [Start] history (P:3)\n",
      "   ‚úÖ [Done] history (2.58s)\n",
      "\n",
      "üéâ All Jobs Completed.\n",
      "   -> [Mock] Job Committed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "# ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á import awsglue ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Cell ‡∏ö‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÅ‡∏•‡πâ‡∏ß\n",
    "\n",
    "# ==========================================\n",
    "# 1. Init Spark & Glue Wrappers\n",
    "# ==========================================\n",
    "sc = SparkContext.getOrCreate() # ‡πÉ‡∏ä‡πâ getOrCreate ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô notebook\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Mock Arguments\n",
    "args = {\n",
    "    'JOB_NAME': 'local_notebook_test',\n",
    "    'MAX_CONCURRENT_THREADS': '3' # ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡πà‡∏ô‡∏î‡∏π‡∏Ñ‡∏£‡∏±‡∏ö\n",
    "}\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Configuration & Mock Data\n",
    "# ==========================================\n",
    "MAX_WORKERS = int(args['MAX_CONCURRENT_THREADS'])\n",
    "S3_BUCKET_ROOT = \"s3a://mock-bucket\" # ‡πÉ‡∏ä‡πâ s3a protocol ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö local spark\n",
    "\n",
    "def get_job_control_list():\n",
    "    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á Data ‡∏ä‡∏∏‡∏î‡πÄ‡∏î‡∏¥‡∏°\n",
    "    return [\n",
    "        {'table_id': 1, 'target_table': 'users', 'piority': 1, 'parallel_run': True},\n",
    "        {'table_id': 2, 'target_table': 'products', 'piority': 1, 'parallel_run': True},\n",
    "        {'table_id': 3, 'target_table': 'orders', 'piority': 2, 'parallel_run': False}, # Sequential\n",
    "        {'table_id': 4, 'target_table': 'logs_a', 'piority': 2, 'parallel_run': True},\n",
    "        {'table_id': 5, 'target_table': 'logs_b', 'piority': 2, 'parallel_run': True},\n",
    "        {'table_id': 6, 'target_table': 'logs_c', 'piority': 2, 'parallel_run': True},\n",
    "        {'table_id': 7, 'target_table': 'history', 'piority': 3, 'parallel_run': True},\n",
    "    ]\n",
    "\n",
    "# ==========================================\n",
    "# 3. Core Logic\n",
    "# ==========================================\n",
    "def process_table(row):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n",
    "    \"\"\"\n",
    "    table_name = row['target_table']\n",
    "    piority = row['piority']\n",
    "    \n",
    "    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Random ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û Parallel)\n",
    "    import random\n",
    "    process_time = random.uniform(1.0, 3.0) \n",
    "    \n",
    "    print(f\"   ‚ñ∂ [Start] {table_name} (P:{piority})\")\n",
    "    time.sleep(process_time) # Simulate Data Processing\n",
    "    \n",
    "    # --- ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏∞‡∏ï‡πà‡∏≠ S3 ‡∏à‡∏£‡∏¥‡∏á ---\n",
    "    # df = spark.createDataFrame(...) \n",
    "    # df.write.parquet(...)\n",
    "    \n",
    "    return f\"   ‚úÖ [Done] {table_name} ({process_time:.2f}s)\"\n",
    "\n",
    "# ==========================================\n",
    "# 4. Orchestration Execution\n",
    "# ==========================================\n",
    "print(f\"--- Starting Job with {MAX_WORKERS} Max Threads ---\\n\")\n",
    "\n",
    "job_list = get_job_control_list()\n",
    "all_priorities = sorted(list(set(job['piority'] for job in job_list)))\n",
    "\n",
    "for prio in all_priorities:\n",
    "    print(f\"\\nüì¢ PRIORITY GROUP: {prio}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    current_batch = [job for job in job_list if job['piority'] == prio]\n",
    "    parallel_jobs = [job for job in current_batch if job.get('parallel_run') == True]\n",
    "    sequential_jobs = [job for job in current_batch if job.get('parallel_run') != True]\n",
    "    \n",
    "    # 4.1 Run Sequential\n",
    "    if sequential_jobs:\n",
    "        print(f\"üëâ Running Sequential ({len(sequential_jobs)} jobs)...\")\n",
    "        for job_row in sequential_jobs:\n",
    "            print(process_table(job_row))\n",
    "            \n",
    "    # 4.2 Run Parallel\n",
    "    if parallel_jobs:\n",
    "        print(f\"üëâ Running Parallel ({len(parallel_jobs)} jobs)...\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            future_to_table = {executor.submit(process_table, job): job for job in parallel_jobs}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_table):\n",
    "                try:\n",
    "                    print(future.result())\n",
    "                except Exception as exc:\n",
    "                    print(f\"‚ùå Error: {exc}\")\n",
    "\n",
    "print(\"\\nüéâ All Jobs Completed.\")\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
